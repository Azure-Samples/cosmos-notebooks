{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Global Distribution with Azure Cosmos DB\n\nIn this notebook we will compare the read and write latency from this Notebook located in Central US between three different Cosmos accounts.\n\nFirst, you must create three Cosmos DB accounts with the following configurations.\n\n- accounts 0: Single-master, single region, East US 2. \n- accounts 1: Single-master, two regions, East US 2 (master replica) and Central US (read replica). \n- accounts 2: Multi-master, two regions, East US 2 (master replica) and Central US (master replica).\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialize Resources\nIn this cell connect to the three accounts, put into an array, then iterate over the array creating a database and container for each account."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "import logging\nlogger = logging.getLogger()\n#logger.setLevel(logging.CRITICAL)\n\nimport azure.cosmos.documents as documents\nimport azure.cosmos.cosmos_client as cosmos\nfrom azure.cosmos.partition_key import PartitionKey\nimport time\n\n#\nacct0_uri = \"<fill-me>\"\nacct0_key = \"<fill-me>\"\nacct0_connection_policy = documents.ConnectionPolicy()\nacct0_connection_policy.PreferredLocations = [\"East US 2\"]\n\nacct1_uri = \"<fill-me>\"\nacct1_key = \"<fill-me>\"\nacct1_connection_policy = documents.ConnectionPolicy()\nacct1_connection_policy.PreferredLocations = [\"Central US\"]\n\nacct2_uri = \"<fill-me>\"\nacct2_key = \"<fill-me>\"\nacct2_connection_policy = documents.ConnectionPolicy()\nacct2_connection_policy.PreferredLocations = [\"Central US\"]\nacct2_connection_policy.UseMultipleWriteLocations = True\n\naccounts = []\n\naccounts.append(cosmos.CosmosClient(url=acct1_uri, auth={'masterKey':acct1_key}, consistency_level=documents.ConsistencyLevel.Eventual, connection_policy=acct1_connection_policy))\naccounts.append(cosmos.CosmosClient(url=acct2_uri, auth={'masterKey':acct2_key}, consistency_level=documents.ConsistencyLevel.Eventual, connection_policy=acct2_connection_policy))\naccounts.append(cosmos.CosmosClient(url=acct3_uri, auth={'masterKey':acct3_key}, consistency_level=documents.ConsistencyLevel.Eventual, connection_policy=acct3_connection_policy))\n\ndb_name = \"db1\"\ncontainer_name  = \"c1\"\ndb_query = \"select * from r where r.id = '{0}'\".format(db_name)\ncontainer_query = \"select * from r where r.id = '{0}'\".format(container_name)\n\nfor account in accounts:\n    # Create the database if it doesn't exist\n    db = list(account.query_databases(db_query))\n    if db:\n        print('Database already exists')\n    else:\n        account.create_database(id=db_name)\n        print('Database created')\n        time.sleep(3)\n    # Create the container\n    db = account.get_database_client(db_name)\n    containers = db.read_all_containers()\n    if(any(container['id'] == container_name for container in containers)):\n        db.delete_container(container_name); #delete and recreate to clear out old data\n        print('delete container')\n    pk = PartitionKey(path='/id', kind='Hash')\n    db.create_container(container_name, pk)\n    print('Container created')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pre-load Data\nIn this cell, pre-load Account 0 and Account 1 with 100 items to prepare them for a read-latency test."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "import sys\nimport json\nimport random\nimport uuid\n\n!{sys.executable} -m pip install Faker --user\nfrom faker import Faker\nfake = Faker()\n\nc0 = accounts[0].get_database_client(db_name).get_container_client(container_name)\nc1 = accounts[1].get_database_client(db_name).get_container_client(container_name)\n\nfor x in range(0, 100):\n    item1 = {\n      \"id\": str(uuid.uuid4()),\n      \"name\": fake.name(),\n      \"city\": fake.city(),\n      \"state\": fake.state(),\n      \"uid\": random.randint(0,100)\n    }\n    item2 = item1\n    c0.create_item(body=item1)\n    c1.create_item(body=item2)\nprint(\"Data load complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Read Latency Benchmark Single-master, Single-region\nThis cell will benchmark the read latency for a Notebook running in Central US against a database with only a single read/write region in East US 2.\n\nThis test will first query the container to get the self id's for 100 items. Then iterate through them and execute 100 point reads, measuring latency and RU cost for each read. It will then print the average latency and RU/s at the end."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "#Load a list of id's to do point reads with\nc = accounts[0].get_database_client(db_name).get_container_client(container_name)\n#%%sql --database {db_name} --container {container_name} --output ids\nsql = \"SELECT value c.id FROM c\"\nids = list(c.query_items(query=sql, enable_cross_partition_query=True))\n\nl = []\nr = []\n\nfor id in ids:\n    start = time.time()\n    response = c.read_item(item=id, partition_key=id)\n    end = time.time()\n    latency = round((end-start)*1000)\n    ru = float(c.client_connection.last_response_headers['x-ms-request-charge'])\n    l.append(latency)\n    r.append(ru)\n    #print(\"Latency: \" + str(latency) + \"ms, RU: \" + str(ru))\n\nl.sort()\nl = l[:99]\navgL = round(sum(l)/len(l))\navgR = round(sum(r)/len(r))\n\nprint(\"Read Latency for Single-Master, Single-Region: Reader in Central US, Read Replica in East US 2\")\nprint(\"Average Read Latency (P99): \" + str(avgL) + \"ms, Average RU: \" + str(avgR))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Read Latency Benchmark Single-master, Multi-region\nThis cell will benchmark the read latency for a Notebook running in Central US against a database with a master replica in East US 2 and a read replica in Central US.\n\nThis test will first query the container to get the self id's for 100 items. Then iterate through them and doing 100 point reads, measuring latency and RU cost for each read. It will then print the average latency and RU/s at the end."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "#Load a list of id's to do point reads with\nc = accounts[1].get_database_client(db_name).get_container_client(container_name)\n#%%sql --database {db_name} --container {container_name} --output ids\nsql = \"SELECT value c.id FROM c\"\nids = list(c.query_items(query=sql, enable_cross_partition_query=True))\n\nl = []\nr = []\n\nfor id in ids:\n    start = time.time()\n    response = c.read_item(item=id, partition_key=id)\n    end = time.time()\n    latency = round((end-start)*1000)\n    ru = float(c.client_connection.last_response_headers['x-ms-request-charge'])\n    l.append(latency)\n    r.append(ru)\n    #print(\"Latency: \" + str(latency) + \"ms, RU: \" + str(ru))\n\nl.sort()\nl = l[:99]\navgL = round(sum(l)/len(l))\navgR = round(sum(r)/len(r))\n\nprint(\"Read Latency for Single-Master, Multi-Region: Reader in Central US, Read Replica in Central US\")\nprint(\"Average Read Latency (P99): \" + str(avgL) + \"ms, Average RU: \" + str(avgR))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Write Latency Benchmark Single-master, Multi-region\n\nThis cell will benchmark the write latency for a Notebook running in Central US against a database with it's write replica in East US 2.\n\nThis test will first generate 100 items to insert. Then iterate through the list and do 100 inserts, measuring latency and RU cost for each write. It will then print the average write latency and RU/s at the end."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "c = accounts[1].get_database_client(db_name).get_container_client(container_name)\n\nl = []\nr = []\nitems = []\n\nfake = Faker()\nprint(\"create 100 items for test\")\nfor x in range(0, 100):\n    item = {\n      \"id\": str(uuid.uuid4()),\n      \"name\": fake.name(),\n      \"city\": fake.city(),\n      \"state\": fake.state(),\n      \"uid\": random.randint(0,100)\n    }\n    items.append(item)\n    \nfor item in items:\n    start = time.time()\n    c.create_item(body=item)\n    end = time.time()\n    latency = round((end-start)*1000)\n    ru = float(c.client_connection.last_response_headers['x-ms-request-charge'])\n    l.append(latency)\n    r.append(ru)\n    #print(\"Write Latency: \" + str(latency) + \"ms, RU: \" + str(ru))\n\nl.sort()\nl = l[:99]\navgL = round(sum(l)/len(l))\navgR = round(sum(r)/len(r))\n\nprint(\"Write Latency for Single-Master, Multi-Region: Writer in Central US, Write Replica in East US 2\")\nprint(\"Average Write Latency (P99): \" + str(avgL) + \"ms, Average RU: \" + str(avgR))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Write Latency Benchmark Multi-master, Multi-region\n\nThis cell will benchmark the write latency for a Notebook running in Central US against a multi-master database in East US 2 and Central US regions.\n\nThis test will first generate 100 items to insert. Then iterate through the list and do 100 inserts, measuring latency and RU cost for each write. It will then print the average write latency and RU/s at the end."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "c = accounts[2].get_database_client(db_name).get_container_client(container_name)\n\nl = []\nr = []\nitems = []\n\nfake = Faker()\nprint(\"create 100 items for test\")\nfor x in range(0, 100):\n    item = {\n      \"id\": str(uuid.uuid4()),\n      \"name\": fake.name(),\n      \"city\": fake.city(),\n      \"state\": fake.state(),\n      \"uid\": random.randint(0,100)\n    }\n    items.append(item)\n    \nfor item in items:\n    start = time.time()\n    c.create_item(body=item)\n    end = time.time()\n    latency = round((end-start)*1000)\n    ru = float(c.client_connection.last_response_headers['x-ms-request-charge'])\n    l.append(latency)\n    r.append(ru)\n    print(\"Write Latency: \" + str(latency) + \"ms, RU: \" + str(ru))\n\nl.sort()\nl = l[:99]\navgL = round(sum(l)/len(l))\navgR = round(sum(r)/len(r))\n\nprint(\"Write Latency for Multi-Master, Multi-Region: Writer in Central US, Write Replica in Central US\")\nprint(\"Average Write Latency (P99): \" + str(avgL) + \"ms, Average RU: \" + str(avgR))\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
