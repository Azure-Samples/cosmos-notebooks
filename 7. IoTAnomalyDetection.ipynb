{"cells":[{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"# Cosmos DB IoT Data Generator\n\nThis notebook will simulate IoT devices, loading data into a Cosmos DB contatiner. If you activate the Analytics Store feature, you can also perform reporting and analytics with [Azure Synapse](https://azure.microsoft.com/en-us/services/synapse-analytics/). You can customize the code as necessary, changing the units, measures, number of devices, and outliers behavior. Tips will and best practices will be shared in the code comments and **\"Did you know?\"** sections. \n\n## Business Scenario\n\nThe hypothetical scenario is Power Plant, where IoT devices are monitoring [steam turbines](https://en.wikipedia.org/wiki/Steam_turbine). The code will created realistic Revolutions per minute (RPM) and MegaWats (MW) data for each turbine. There is one device for each turbine. Each one of the measured units, RPM and MW,  have a base value and a variation. The process will create one data point per second per unit per turbine (or device). \n\n&nbsp;\n\n<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/iot-ai-notebook-1.png\" alt=\"Built-in nteract \" width=\"50%\"/>\n\n&nbsp;\n\n\nThere will be one outlier per minute, in random frequency. In those situations, RPM values will go up and MW output will go down, because of the circuit protection of the system. The idea is to see the data varying at the same time, but with different signals. Suggested analytics scenarios are [Predictive Maintenance](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/predictive-maintenance-playbook) and [Anomally Detection](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/apps-anomaly-detection-api).\n\n\n## Technical Information\n\n+ The data will be uploaded direclty to a Cosmos Db container.\n+ We will create a database and a container, with 400 RU/s. In Cosmos Db, each write uses 5 RUs for each 1 KB. \n+ Because of the 400 RU/s, we will simulate up to 20 IoT devices at the same time. Each one of them will create 2 data points (RPM and MW) per second. For more deails, check the **3.RequestUnit** notebook.\n+ We will create 2 containers, both will be partitioned per deviceId.\n+ Each measured unit (RPM and MW) has a base value and a variation percentage, that can be positive or negative.\n+ The code runs for the number of minutes you want or until you stop it. Minimum value is 1 minute.\n+ There will be 1 random outlier per 1 minute. For 2+ minutes executions, the outliers may happen in sequence or not.\n+ All devices have outliers at the same time, but with different variations.\n+ Outliers will have a bigger base value and a bigger variation.\n+ This code is not production grade, it was created as a demo and tutorial. In a production scenario, this notebook code should be enriched with error handling, [global data distribution](https://docs.microsoft.com/en-us/azure/cosmos-db/distribute-data-globally), etc. \n\n&nbsp;\n\n\n>**Did you know?** Cosmos Db is a great fit for IoT workloads. Click [here](https://docs.microsoft.com/en-us/azure/cosmos-db/use-cases) to learn more about Cosmos Db recommended use cases.\n"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## 1st Step - Initialization \n\nWe will start by creating, if necessary, the database and the container. To connect to the service, you can use our built-in instance of ```cosmos_client```. This is a ready to use instance of [CosmosClient](https://docs.microsoft.com/python/api/azure-cosmos/azure.cosmos.cosmos_client.cosmosclient?view=azure-python) from our Python SDK. It already has the context of this account baked in.\n\n&nbsp;\n\nWe will also create:\n+ The function that generates the elementary iot values\n+ The main function, the calls the 1st function and saves the data in a Cosmos Db container\n\n&nbsp;\n\n>**Did you know?** If you run the ```dir()``` python command, you will see which libraries are loaded. And ```json```is pre-loaded.\n\n&nbsp;\n\n>**Did you know?** The word \"value\" is a reserved, what will impact your queries. That's why we are calling the values as ***measureValue***."},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"<DatabaseProxy [dbs/AnalyticsDb]> ok\n<ContainerProxy [dbs/AnalyticsDb/colls/IotData]> ok\nFunctions retunrIotValues ok\nFunction iotSimulator ok\n"}],"source":"import random\nimport datetime\nimport time\nimport azure.cosmos as cosmos\nimport azure.cosmos.exceptions as exceptions\nfrom azure.cosmos.partition_key import PartitionKey\nimport uuid\n\n\n\n#  Initialization\ndb_name = \"AnalyticsDb\"\ncontainer_name = \"IotData\"\npartition_key_value = \"/deviceId\"\n\n# Key Objects Creation\ndatabase_client = cosmos_client.create_database_if_not_exists(db_name)\nprint(database_client, 'ok')\n\ncontainer_client = database_client.create_container_if_not_exists(id=container_name, partition_key=PartitionKey(path=partition_key_value),offer_throughput=400)\nprint(container_client, 'ok')\n\n###########################################################################################\n# The function that creates and returns IoT values\n###########################################################################################\ndef retunrIotValues(deviceId, measureType, unitSymbol, unit, baseValue, variationPercentage,isOutlier,outlierSignal):\n    if (isOutlier == 0):\n        value = random.randint(int(baseValue - (baseValue * (variationPercentage)/100)), int(baseValue + (baseValue * (variationPercentage)/100 )))\n    else: #Outlier!\n        if (outlierSignal == 'Positive'):\n            value = random.randint(int(baseValue), int(baseValue + (baseValue * (variationPercentage)/100 )))\n        else:\n            baseValue = int(baseValue/2) #to fix to 30% increase in the main function. \n            value = random.randint(int(baseValue - (baseValue * (variationPercentage)/100)), int(baseValue))\n\n    docId = str(uuid.uuid4())\n\n    iotData = {\n    'id' : docId,\n    'dateTime' : datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    'deviceId' : deviceId,\n    'measureType' : measureType,\n    'unitSymbol' : unitSymbol,\n    'unit' : unit,\n    'measureValue' : value\n    }\n\n    return iotData\n\n\nprint('Functions retunrIotValues ok')\n\n###########################################################################################\n# The Main function\n###########################################################################################\ndef iotSimulator(numDevs=40, minutes=1, printOutput=0): \n\n    # Initial validations\n    \n    if (numDevs > 40):\n        print (\"First parameter: Too many devices, maximum = 40\")\n        return\n    \n    if (minutes == 0):\n        print (\"Second parameter: Minimum minutes = 1\")\n        return\n\n    if (printOutput != 1) and (printOutput != 0):\n        print (\"Third parameter: Only 1 (yes) and 0 (no) are accepted for terminal output printing\")\n        return\n\n    # Devices list, startint with dev-1\n    devPrefix = 'dev-'\n    devicesList = [ ] \n    for i in range(1,numDevs+1):\n        deviceId = devPrefix + str(i)\n        devicesList.append(deviceId)\n\n    # Units list\n    # This is a Power Plant Scenario. Chage as you want\n    # layoyt is: measureType, unitSymbol, unit,baseValue, variationPercentage, outlierSignal\n    # reference: https://www.mhps.com/products/steamturbines/lineup/thermal-power/1200/\n    unitList = [('Rotation Speed','RPM','Revolutions per Minute',3000,10, 'Positive'), ('Output','MW','MegaWatts',1500,10,'Negative')]\n\n    \n    # How many measures based on the number per minutes?\n    numberMeasures = int(minutes)*60 \n    accNumberMeasures = 0\n    \n    # Outliers\n    accOutliers = 0\n    ouliersSet = set()\n    while len(ouliersSet) < int(minutes):\n        outlier = random.randint(1, int(minutes)*60)\n        ouliersSet.add(outlier)\n    print ('Starting the process. The outlier(s) will happen at: ',ouliersSet)\n    \n    # Create IoT Values based on a base value and a variation. Every device will return 1 value per unit per second.\n    # Data is printed and saved into a Cosmos DB Container\n    # Data modeling: We could have one document per device. But this approach is addressed in the data modeling notebook.\n    while (accNumberMeasures <= numberMeasures):\n        time.sleep(1)\n        for deviceId in devicesList:\n            for unit in unitList:\n                if (accNumberMeasures in ouliersSet):  # If yes, time for outliers, 30% bigger!\n                    iotData = retunrIotValues (deviceId,unit[0],unit[1],unit[2],unit[3]*1.3,unit[4]*1.3,1,unit[5])\n                    print('Outlier:',deviceId,unit[2])\n                else: # Regular measure\n                    iotData = retunrIotValues (deviceId,unit[0],unit[1],unit[2],unit[3],unit[4],0,unit[5])\n                container_client.create_item(body=iotData)\n                if (printOutput == 1):\n                    print(iotData)\n        accNumberMeasures +=1 \n\n        \nprint ('Function iotSimulator ok')\n"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 2 - Running the Data Generator\n\n+ 1st parameter is the number of devices.\n+ 2nd parameter is the number of minutes.Minimum is 1 minute.\n+ 3rd parameter is a boolean, if you want to print the IoT Data to your terminal. It is a good idea to turn it off for +5 minutes executions."},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Starting the process. The outlier(s) will happen at:  {43}\n{'id': '92a4a27d-eb2c-45a1-9291-b6c1fe4f1987', 'dateTime': '2020-04-09 18:53:23', 'deviceId': 'dev-1', 'measureType': 'Rotation Speed', 'unitSymbol': 'RPM', 'unit': 'Revolutions per Minute', 'measureValue': 2744}\n{'id': 'c0da7759-0f6a-4b12-9bd1-8d12f50f1544', 'dateTime': '2020-04-09 18:53:23', 'deviceId': 'dev-1', 'measureType': 'Output', 'unitSymbol': 'MW', 'unit': 'MegaWatts', 'measureValue': 1498}\n{'id': '9420bb35-8365-4de8-91a5-9a0ada2d1e26', 'dateTime': '2020-04-09 18:53:23', 'deviceId': 'dev-2', 'measureType': 'Rotation Speed', 'unitSymbol': 'RPM', 'unit': 'Revolutions per Minute', 'measureValue': 2972}\n{'id': 'af815b27-0ca0-470c-a7a0-90ecbcd82ac3', 'dateTime': '2020-04-09 18:53:23', 'deviceId': 'dev-2', 'measureType': 'Output', 'unitSymbol': 'MW', 'unit': 'MegaWatts', 'measureValue': 1593}\n{'id': '68ee0bae-3db1-4f72-8e92-50e331ee59fc', 'dateTime': '2020-04-09 18:53:24', 'deviceId': 'dev-1', 'measureType': 'Rotation Speed', 'unitSymbol': 'RPM', 'unit': 'Revolutions per Minute', 'measureValue': 3084}\n{'id': '4fd49ac9-f729-47b3-9f99-12a7e12a4f8f', 'dateTime': '2020-04-09 18:53:24', 'deviceId': 'dev-1', 'measureType': 'Output', 'unitSymbol': 'MW', 'unit': 'MegaWatts', 'measureValue': 1393}\n{'id': '9b27a232-4865-4b16-a2d2-abf9fc519642', 'dateTime': '2020-04-09 18:53:24', 'deviceId': 'dev-2', 'measureType': 'Rotation Speed', 'unitSymbol': 'RPM', 'unit': 'Revolutions per Minute', 'measureValue': 3232}\n{'id': 'c7803cea-84b8-4101-bb01-53d60f9070a4', 'dateTime': '2020-04-09 18:53:24', 'deviceId': 'dev-2', 'measureType': 'Output', 'unitSymbol': 'MW', 'unit': 'MegaWatts', 'measureValue': 1625}\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9e1fa28bbdf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miotSimulator\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-a99d63893929>\u001b[0m in \u001b[0;36miotSimulator\u001b[0;34m(numDevs, minutes, printOutput)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Data modeling: We could have one document per device. But this approach is addressed in the data modeling notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccNumberMeasures\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnumberMeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdeviceId\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevicesList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munitList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":"iotSimulator (2,1,1)"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 3 - Analytics\n\nLet's analyze the data using multiple statistics functions and visualizations. And everything starts with a query, loading 1000 metrics into a data frame. Why 1000? Because it is enough for our objectives with this notebook, but you can change it to any number up to 8046. This is a limitation of the Anomaly Detector API. \n\nNow we will:\n\n&nbsp;\n\n+ Use [Cosmos DB SQL API](https://docs.microsoft.com/en-us/azure/cosmos-db/sql-query-getting-started) to load data from the database into a data-frame.\n+ Vizualize and manipulate the data with [Pandas](https://pandas.pydata.org/) and [matplotlib](https://matplotlib.org/) Python libraries.\n+ Apply AI to our data, using the [Anomaly Detector API](https://azure.microsoft.com/en-us/services/cognitive-services/anomaly-detector) to extract hidden insights from our data.\n+ Load the results back to Cosmos Db, to persist the data and be able to create Power BI dashboards.\n\n&nbsp;\n\n>**Did you know?** You can set the default database and container context for all new queries using\n ```%database {database_id}``` and ```%container {container_id}``` syntax. But they need to be in separated cells, as the ```%%sql``` magic command.\n \n &nbsp;\n\n>**Did you know?** All steps from now one are executed for only one device, the first one (dev-1). This notebook is intended to teach how to use all this capabilities. In a loop, it would not be possible to teach as we run the code, we can't add markdown in the middle of Python cells. Everything starts with the query below, filtering the deviceId. In a \"production scenario\", all of them would be loaded and the rest of the could should be in a loop."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"%database AnalyticsDb"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"%container IotData"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"%%sql  --output df_IotData\nSELECT top 1000 c.dateTime, c.unitSymbol, c.measureValue, c.deviceId FROM c"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 4 - Built-in Data Visualizations\n\nWe'll run queries and use the built-in [nteract data explorer](https://blog.nteract.io/designing-the-nteract-data-explorer-f4476d53f897) to allow data visualizations. After the following code lines, all cells will be enabled with nteract data explorer, with multiple visualization options on the right side of the result sets. For more information, check the **2.Visualization** notebook.\n\n&nbsp;\n\n``pd.options.display.html.table_schema = True``\n\n``pd.options.display.max_rows = None``  \n\n&nbsp;\n\nYou will notice that, despite the great charts options, we can't use all possibilities because the units values are mixed in the same column. To fix it, we will filter the data per unit. Chage it as you want and get confortable with the visualization optoins. One suggestion is the **line chart**, where you can use multiple extra visualizations like **stacked area chart**. \n\n&nbsp;\n\n>**Did you know?** After we run the nteract options, only the last data output command will have its data shown. That's why if you want more than one date-out command, you'll need one cell for each.\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"# Turniung on the visualization options\npd.options.display.html.table_schema = True\npd.options.display.max_rows = None\n\n# Example: let's see what we loaded from Cosmos Db\n# df_IotData.head(10)\ndf_IotData[(df_IotData.unitSymbol == \"RPM\") & (df_IotData.deviceId == \"dev-1\")]\n"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 5 - Data Preparation and Advanced data Visualization\n\nBut filtering is not ideal and many other data manipulations will be necessary to reach our objective. That's why we will do some data preparation in the next steps. We need to pivot the data to have one column for each metric, avoiding the mistake to compare different measures. This is also a required process for data science projects, a process called featurization. All columns can be used as input for a machine learning model, that's why the table must be pivoted. \n\n&nbsp;\n\nAlso, we need to remove the rows where we have missing data, what can be caused by the moments when one metric is created in one second and the others in the next second, as you can see in the image below, a real snapshot of the data we just created using this generator.\n\n&nbsp;\n\n<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/iot-ai-notebook-2.PNG\" alt=\"Built-in nteract \" width=\"75%\"/>\n\n&nbsp;\n\n\n>**Did you know?** Data preparation is a [key phase](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-modeling) of data science projects and may include [data engineering](https://docs.microsoft.com/en-us/learn/certifications/roles/data-engineer), [data featurization](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/data-transformation-learning-with-counts), [data cleaning (or cleansing)](https://en.wikipedia.org/wiki/Data_cleansing), and more. It may take up to 80% of the all project!\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n###########################################################################################\n# Data Preparation\n###########################################################################################\n\n# Lets keep df_IotData as is, so we will never change it. All changes will happen in the new data frame.\n# Pivoting the original data for 1 device into a new one, with one column per unit.\nnew_df = df_IotData[(df_IotData.deviceId == \"dev-1\")]\nnew_df=new_df.pivot(index='dateTime', columns = 'unitSymbol' , values =  'measureValue')\n\n# Removing lines with missing data. \nnew_df=new_df.dropna()\n\n# The pivot will create a df where the dataTime column is the index. \n# But for the Anomaly detection, we need a column with that content called timestamp\nnew_df['timestamp']=new_df.index\n\n# We also need a sequential id, to join the dataframes in the end of the process\nnew_df['index']=list(range(len(new_df)))\n\n# Now let's make the sequential the index of the dataframe\nnew_df.set_index('index',inplace=True)\n\n\n###########################################################################################\n# Plotting the Data\n###########################################################################################\n# One chart per unit.\n# If you change or add the units, from now on you need to customize the code.\nnew_df.plot(y='MW', x= 'timestamp', color='green',figsize=(20,5), label = 'Output MW')\nplt.title('MW TimeSeries')\nnew_df.plot(y='RPM', x= 'timestamp', color='red', figsize=(20,5), label = 'RPM')\nplt.title('RPM TimeSeries')\nplt.legend(loc = 'best')\nplt.show()\n"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 6 - Anomaly Detection\n\nThe chart above are very clear about the outliers. But can we predict them? Are there variation margins? Let's use AI to do it, leveraging the [Microsoft Azure Anomaly Detector API](https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/quickstarts/detect-data-anomalies-python?tabs=linux).\n\n&nbsp;\n\nYou must have a [Cognitive Services API account](https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Clinux) with access to the Anomaly Detector API. You can get your subscription key from the Azure portal after creating your account.\n\n&nbsp;\n\nThe API accepts json files with 2 elements, ```timestamp``` and ```value```. These names are mandatory and we can't mix the units for detection, that's why we create 2 customized data frames in the code below, right before the Anomaly Detection Function execution, one for each unit. Also, we should avoid to mix the devices in the same analysis, since they may have different behavior.\n\n&nbsp;\n\nThe output includes, for each unit, the expected value, the lower and upper margin values, and if it was an anomaly or not. With the margins, you can monitor in real time if a value is gettig close to the margins. Anomaly is when a value cross them. \n\n&nbsp;\n\n**DON'T FORGET TO ADD YOUR ANOMALY DETECTOR API KEY TO THE CODE BELOW!! JUST SEARCH FOR 'paste-your-key-here' AND REPLACE THE TEXT WITH YOUR KEY, THAT YOU CAN GET/COPY FROM THE AZURE PORTAL.**\n\n&nbsp;\n\n>**Did you know?** Anomaly Detector API has 2 endpoints, for 2 different cappabilities: Real time and Batch detection. Real time will detect anomalies for every 10 data points. The Batch module will analyze up to 8046 datapoints, and since we already have the data, that's what we will use.\n\n&nbsp;\n\n>**Did you know?** An Azure Cognitive Services account gives you access to multiple cognitive services like Aext Analytics and Computer Vision APIs.\n\n&nbsp;\n\n>**Did you know?** \"Secondly\" granularity is not documented yet, but it works and that's exactly what we need for this analysis.\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"import http.client, urllib.request, urllib.parse, urllib.error, base64\n\n###########################################################################################\n# The Anomaly Detection Function\n###########################################################################################\ndef anomalyDetector (myJson):\n    headers = {\n        # Request headers\n        'Content-Type': 'application/json',\n        'Ocp-Apim-Subscription-Key': 'paste-your-key-here', # <<---- Paste your key here!!!!\n    }\n\n    params = urllib.parse.urlencode({\n    })\n   \n    \n    try:\n        conn = http.client.HTTPSConnection('westus2.api.cognitive.microsoft.com')\n        conn.request(\"POST\", \"/anomalydetector/v1.0/timeseries/entire/detect?%s\" % params, myJson, headers)\n        response = conn.getresponse()\n        data = response.read()\n        return(data)\n        conn.close()\n    except Exception as e:\n        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n \n\nprint ('Function anomalyDetector() ok')\n###########################################################################################\n# The main funciton, that will process per device\n###########################################################################################\n\n# Let's use the same data frame of the first analysis to get data the devices ids.\ndevicesArray = df_IotData.deviceId.unique()\n\n# Creating Output dataframe\noutput_df = pd.DataFrame()\n\n# Let's process per device\nfor deviceId in devicesArray:\n    print ('Processing device: ',deviceId)\n    \n    # Same data preparation we had for data visualization\n    new_df = df_IotData[(df_IotData.deviceId == deviceId)]\n    # The pivot below will remove deviceId column. We will add it back later.\n    new_df=new_df.pivot(index='dateTime', columns = 'unitSymbol' , values =  'measureValue')\n    new_df=new_df.dropna()\n    \n    # The pivot will create a df where the dataTime column is the index. \n    # But for the Anomaly detection, we need a column with that content called timestamp\n    new_df['timestamp']=new_df.index\n\n    # We also need a sequential id, to join the dataframes in the end of the process\n    new_df['index']=list(range(len(new_df)))\n\n    # Now let's make the sequential the index of the dataframe\n    new_df.set_index('index',inplace=True)\n\n    #### RPM\n    # Creating a dataframe\n    new_df_RPM=new_df[['timestamp','RPM']]\n    new_df_RPM.rename(columns={\"RPM\":\"value\"},inplace=True)\n    # Creating json string\n    myJson= '{\"granularity\":\"secondly\",\"series\": '\n    myJson = myJson+ new_df_RPM.to_json(orient = 'records')+'}'\n    # Running and saving the results in a new dataframe\n    rpmAnomalies= anomalyDetector (myJson)\n    new_df_RPM_results = pd.read_json(rpmAnomalies)\n    \n    ### MW\n    # Creating a dataframe\n    new_df_MW=new_df[['timestamp','MW']]\n    new_df_MW.rename(columns={\"MW\":\"value\"},inplace=True)\n    # Creating json string\n    myJson= '{\"granularity\":\"secondly\",\"series\": '\n    myJson = myJson+ new_df_RPM.to_json(orient = 'records')+'}'\n    # Running and saving the results in a new dataframe\n    mwAnomalies= anomalyDetector (myJson)\n    new_df_MW_results = pd.read_json(mwAnomalies)\n\n    # Checking the results. Number of rows should be exactly the same.\n    rpmCount = new_df_RPM_results.shape[0]  # gives number of row count\n    mwCount = new_df_MW_results.shape[0]  # gives number of row count\n\n    if (rpmCount != mwCount):\n        print('Something went wrong, total rows in the results should be the same.')\n        print ('rpmCount: ',rpmCount)\n        print ('mwCount: ',mwCount)\n    else:\n        # Preparing the margin values for plotting\n        new_df_RPM_results['lowerMargins'] = new_df_RPM_results['expectedValues'] - new_df_RPM_results['lowerMargins']\n        new_df_RPM_results['upperMargins'] = new_df_RPM_results['expectedValues'] + new_df_RPM_results['upperMargins']\n        new_df_MW_results['lowerMargins'] = new_df_MW_results['expectedValues'] - new_df_MW_results['lowerMargins']\n        new_df_MW_results['upperMargins'] = new_df_MW_results['expectedValues'] + new_df_MW_results['upperMargins']\n        # Merging original (we need the timestamp/datetime) + results\n        new_df_RPM = new_df_RPM.join(new_df_RPM_results,lsuffix='Original',rsuffix='Results')\n        new_df_MW = new_df_MW.join(new_df_MW_results,lsuffix='Original',rsuffix='Results')\n        # Merging in temp dataframe just to add the deviceId\n        temp_df = new_df_RPM.join(new_df_MW,lsuffix='RPM',rsuffix='MW')\n        temp_df['deviceId'] = deviceId\n        if (output_df.size == 0): #First execution\n            output_df = temp_df\n        else:\n            output_df = output_df.append(temp_df)\n    print ('Rows processed: ',mwCount,' for device ',deviceId)        "},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 7 - Final Data Vizualization\n\nNow let's use the built-in visualizations to analyze the data. The suggestion is to test all options, so you can analyze all data we created using data preparation and AI.\n\n&nbsp;\n\n\n>**Did you know?** All resulted properties from the Anomaly Detector API are listed [here](https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cognitiveservices.anomalydetector.models.entiredetectresponse?view=azure-dotnet-preview). You may want to check this information to create better visualizations."},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"### Final Data Visualization\n\nYou should be able to see interesting line charts, like this image below. Compare all 4 coluns for each unit separated, that't why we pivoted the data. Please notice that in many situations the original value is beyond the margins. The expected value is the base line and you can also check how it compares with the original value.\n\n&nbsp;\n\n<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/iot-ai-notebook-3.PNG\" alt=\"Built-in nteract \" width=\"100%\"/>\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"output_df.head(100)\n"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 8 - Loading the Metadata into Cosmos DB\n\nNow let's load the results into Cosmos Db. **But Why?** For two reasons:\n\n&nbsp;\n\n1. **Metadata:** Now we have the expected values, per device and per unit: the values between the lower and the upper margins. You can read from Cosmos DB and detect anomalies in real time or in batch. This method is cheaper and faster than using the The Anomaly Detector API for every single datapoint. The API should be used from time to time to refresh the metadata because of sasonality and other factors. The API also can be used to cross validate the anomalies detected.\n\n&nbsp;\n\n2. **Advanced Analytics/AI:** This data can be queried not only from Cosmos DB transactional store, but also from our brand new [Analitics Store](https://azure.microsoft.com/en-us/updates/new-analytics-storage-for-azure-cosmos-db-is-now-in-preview/). You can use both for PBI reports, more Machine Learning analysis, predictions, etc.\n\n&nbsp;\n\n>**Did you know?** Cosmos DB guarantees less than 10-ms latencies for both, reads (indexed) and writes at the 99th percentile, all around the world. For more information, click [here](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction#guaranteed-low-latency-at-99th-percentile-worldwide).\n\n### The loading Function\n\nNow let's create a container, if necessary, and load the Anomalies Detection metadata. \n\n&nbsp;\n\n**THIS FUNCTION DOESN'T TEST IF THE SAME DATA WAS UPLOADED BEFORE.**"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"#  Initialization\ndb_name = \"AnalyticsDb\"\ncontainer_name = \"AdMetadata\"\npartition_key_value = \"/deviceId\" #Thinking that in the future you will have multiple devices.Also, data will probably analyzed per device.\n\n# Key Objects Creation\n\ndatabase_client = cosmos_client.create_database_if_not_exists(db_name)\nprint(database_client, 'ok')\n\ncontainer_client = database_client.create_container_if_not_exists(id=container_name, partition_key=PartitionKey(path=partition_key_value),offer_throughput=400)\nprint(container_client, 'ok')\n\nfor i in range(0, len(output_df)):\n    \n    if (i % 10 == 0 ) and (i > 0): #let's report the progress for each 10 docs uploaded\n        print ('Number of docs uploaded: ', i)\n   \n    adMetaData = {\n    'id' : str(uuid.uuid4()),\n    'deviceId' :output_df.iloc[i]['deviceId'], \n    'timestamp' : output_df.iloc[i]['timestampRPM'], #Doesn't matter, for this column RPM = MW\n    'originalMWValue' : int(output_df.iloc[i]['valueMW']),\n    'expectedValuesMW' : int(output_df.iloc[i]['expectedValuesMW']), # Keeping 'values', plural as the original name.\n    'isAnomalyMW' : str(output_df.iloc[i]['isAnomalyMW']).lower(),\n    'isNegativeAnomalyMW' : str(output_df.iloc[i]['isNegativeAnomalyMW']).lower(),\n    'isPositiveAnomalyMW' : str(output_df.iloc[i]['isPositiveAnomalyMW']).lower(),\n    'lowerMarginsMW' : int(output_df.iloc[i]['lowerMarginsMW']),\n    'upperMarginsMW' : int(output_df.iloc[i]['upperMarginsMW']),\n    'periodMW' : int(output_df.iloc[i]['periodMW']),\n    'originalRPMValue' : output_df.iloc[i]['valueRPM'],\n    'expectedValuesRPM' : output_df.iloc[i]['expectedValuesRPM'], # Keeping 'values', plural as the original name.\n    'isAnomalyRPM' : str(output_df.iloc[i]['isAnomalyRPM']).lower(),\n    'isNegativeAnomalyRPM' : str(output_df.iloc[i]['isNegativeAnomalyRPM']).lower(),\n    'isPositiveAnomalyRPM' : str(output_df.iloc[i]['isPositiveAnomalyRPM']).lower(),\n    'lowerMarginsRPM' : int(output_df.iloc[i]['lowerMarginsRPM']),\n    'upperMarginsRPM' : int(output_df.iloc[i]['upperMarginsRPM']),\n    'periodRPM' : int(output_df.iloc[i]['periodRPM'])\n    }\n    container_client.create_item(body=adMetaData)"},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Step 9 - Checking the Metadata Container\n\nLet's finish with a query to count the number of documents we uploaded to the container."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"%container AdMetadata"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"%database AnalyticsDb"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"trusted":true},"outputs":[],"source":"%%sql \nSELECT count(1) FROM c "},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":"## Next Steps\n\nSuggested next steps are:\n\n+ Use Power BI to create dashboards on top of BOTH containers we created. Their combination is very powerful.\n+ Try our other Sample Notebooks.\n+ Send us your feedback or contribution in our [GitHub repo](https://github.com/Azure-Samples/cosmos-notebooks)."}],"metadata":{"kernelspec":{"displayName":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":"ipynb","mimetype":"application/json","name":"python","version":"3.7"},"nteract":{"version":"dataExplorer 1.0"}},"nbformat":4,"nbformat_minor":4}